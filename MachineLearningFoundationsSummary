#机器学习基石学习小结
`颜金洲`

##引言

至此，台湾大学的林轩田老师的《机器学习基石》课程，总共16个lecture，已经全部学完。其中有一段时间工作也是特别的忙，加班较多，导致业余时间较为紧张。这里还是非常庆幸一直坚持到了最后，这个课程让我学习到了机器学习有用的知识，仿佛一扇大门为我敞开；这与我在刚开始学习时，自认为有所了解不可同日而语。

这里，对这门课程做一个简单的小结与回顾。

##课程说了哪些事情
这个课程非常有条理，课程在一开始，便给出了四个问题：

* When Can Machines Learn？
* Why Can Machines Learn？
* How Can Machines Learn？
* How Can Machines Learn Better？

后续的课程都是围绕这四个问题，由浅入深、深入浅出、娓娓而谈。这四个问题，也就是机器学习的适用范围、原理本质、基本方法、调优与原则。

换句话说，这个课程一直都是围绕机器学习的基本概念和原则展开，并没有一直在堆砌机器学习技法。实际上，通过这个课程的学习，我逐渐也了解到了林老师的苦心：*只有打好基础，抓住本质，避免误用、误区，才能合理使用各种技法*。如果仅仅模仿几个招式，没有理解招式本质，是成不了武功高手的。

我现在离武功高手还是有一定距离，但是还是学了几招：

* `什么是机器学习`有了一个直观的了解。
* 粗略`评估某个问题的复杂度`，对复杂度、计算量、效果等有个大致的估算。
* 学习了`三种机器学习技法`。
* 避免`常见的误区`，避免overfitting
* 如何正确、稳妥地`开展机器学习`工程。

接下来，我一一回顾我学到的这几招。

##什么是机器学习

###定义
机器学习是个很时髦的词语，但是这个概念的提出却是和计算机的发明是同一时期的。关于机器学习，百度百科上有较为概述的定义：

>机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。

课程中，林老师给出了他对机器学习，较为技术的定义，我这里做一个转述，但是不保证将林老师的定义说清楚了（正确了）：

* 机器学习的目的是在解决问题，这样的问题都可以抽象成: 给一个输入，需要获得一个正确的输出。我们的程序的目标就是尽量正确的做这件事情。
* 从输入集合到输出集合的一中映射方式称为一个函数，那么所有的映射方式可以称为一个可选的函数集合`H`。
* 把理论上存在一个完美的函数`f`，它对任何合法输入，在没有噪音的情况下，都能取得正确的输出。
* 现实是，我们无法获得完美函数`f`。但是可以通过算法，从已有的资料出发，获取尽量逼近`f`的函数`g`，这个`g`相当于是从`H`中“选”出来的。
* 我们用这个函数`g`来消化未知的输入，获得尽量正确的输出。获取函数`g`的算法就是机器学习过程。

从感性上来说，我们并没有（不能）定义如何处理问题的规则，但是我们通过算法，从数据中获得了处理问题的规则。

###必要性
其实我觉得从定义上，其实没有说机器学习的必要性。机器学习到底是在解决什么问题？有什么问题是必须机器学习才能搞定的？为什么机器学习这么重要？如果不追究细节，我觉得可以用一句话回答：

*机器学习是在解决只有机器能搞定的事情。*

当然，这句话有很大的毛病，我不打算追究这句话的毛病。什么东西人搞不定？

* 数据量到了一定的程度，人脑已经无法处理/给出计算规则。
* 输入和输出没有人脑可以理解的规则。
* 时刻都在变化，人类没有这样的能力去频繁调整。

我只举出了这几个，可能还有很多，机器学习的应用已经非常广泛了。

###机器学习分类（概念）
如果对一些机器学习的基本概念都不了解的话，那怎么沟通？这里给出课程中提到的概念,这些也正是`lecture 3`介绍的内容。机器学习可以按照以下维度进行分类：

* 按照问题输出来分：
	* 二元分类问题：输出yes/no
	* 多元分类问题：输出多个离散的数值
	* 回归问题：输出集合可能整个实数集，或者一个区间。
	* 组合问题：多个问题组合
* 按照学习方式
    * 监督式（supervised）带标记数据
    * 非监督式（unsupervised）没有标记数据
    * 半监督式（semi-supervised）只标记了部分数据
    * 增强式（Reinforcement）个人觉得与“监督”的标记数据最大的区别就是，只是标记某一个判断结果是否正确，而不直接给出正确结果。
* 按照数据获取方式
	* 批量学习（Batch Learning）
	* 实时学习（Online Learning）
	* 交互式学习（Active Learning）
* 输入数据类型
	* concrete 直接相关的属性。sophisticated (and related) physical meaning
	* raw 相关性比较简单/原始。（图像、语音输入）simple physical meaning
	* abstract 相关性不直接。no (or little) physical meaning

另外，还有两个重要概念需要提及：

* `E(in)`: 当前模型对训练数据集进行评估，输出结果的错误率。
* `E(out)`: 当前模型对（未经训练的）测试数据进行评估，输出结果的错误率。

`E(in)`是非常重要的，是训练模型不断优化变好的一个度量标准，是模型训练动力。这个表现为如果还拿之前已经有答案的问题来问我们的模型，回答能否正确。如果要说某次训练的效果不错，起码`E(in)`不能太低，这个是非常容易理解的。

但是林老师想要强调的是`E(out)`同样重要，甚至更加重要。机器学习的最终目标，就是要做到非常小的`E(in)`和`E(out)`。后面的非常多的lecture都在论述如何让`E(out)`接近`E(in)`,并从理论上证明了，`E(out)`和`E(in)`非常接近的条件。这也是这个课程索要表达的核心思想。

##评估问题的复杂度
林老师花了了多个lecture，从浅入深的给出了机器学习基石的一个核心概念`VC（Vapnik-Chervonenkis） bound`。它从数学上定义了机器学习的可能性、机器学习的复杂度、输入数据量与学习效果的关系、训练数据错误率和测试数据错误率的关系（以及后面的重要概念overfitting）。为了引出这个概念，费了如下周折：

1. `lecture 4` 问题能否用机器学习来解决，训练数据错误率是否接近测试数据的错误率。
2. `lecture 5` 引出break point概念，给出了训练数据与可选函数集合`H`的关系。
3. `lecture 6` 通过数学分析，引出VC bound，通过统计推导，证明了数据量、数据的相关性和机器学习的效果的关系（为overfitting做了铺垫）。
4. `lecture 7` 给出VC Dimension，表征某个机器学习的复杂度，以及需要的数据量，并给出了机器学习的错误曲线（后续会强调的误区：并非模型越复杂越好）。

这几节都没有提如何将`E(in)`做的足够小，而是在强调什么情况下，`E(out)`与`E(in)`是非常接近的。在这样的场景下进行机器学习训练是非常安全的。另外，同时对机器学习某个机器学习的复杂度给出了一个评估手段。


##机器学习技法

###基本技法
书中并没有介绍很多机器学习技法，主要说了三种：

* PLA/Pocket：
	* 优点：效率较高、理论完善。
	* 缺点：在有噪音或者不是线性可分的情况下，只能使用效率较低的pocket算法。
	* 原理：不断的找出出错点，然后更新向量`w`，通过数学证明，更新`w`的操作是有限次的。
	* 个人理解：我有一个直观的理解，将数据集的边界划分成两条直线（线段），`w`越接近这两个边界，他们的法向量的方向会越接近，每次调整的幅度会越小，而每次调整都是向着正确的方向调整的。
	* 结论：对于线性可分的场景效果非常好，对于有噪音和线性不可分的场景性能下降较多。
* 线性回归 linear Regression
	* 优点：直接解方程获得，效率高
	* 缺点：效果较弱，较为针对线性回归问题。主要是因为，如果某个点距离分界面的距离较远，会带来较高的错误代价，使得分界面偏离。
	* 原理：相当于解多元线性方程，使得分界面与各点的距离（非严格垂直距离）总和最小。
	* 结论：只有数据抖动不大、数据较为均匀、奇异点较少的情况下使用。因为效率高，一般用于其他技法的初始位置。
* 逻辑回归 logic Regression
	* 优点：理论完备，有成熟的求解方式，效果较好，带置信度方便策略使用。
	* 缺点：效率不算很高，
	* 原理：通过`θ`函数和似然估计，构建一个代价方程，并证明这个代价方程是一个凸的。通过求偏导数进行梯度下降，让代价方程的值接近最低点。
	* 个人理解：相对于线性回归的代价来说，`θ`函数在靠近边界时，权值较大，在远离正确边界时，代价较小，远离错误边界时，代价较大，但是远远低于线性回归的错误代价。因此它保证了分界面接近真实分界面。
	* 结论：相当于软边界分类，越靠近边界，给出的概率越低，越远离边界，给出的置信度越高。

###多元分类

假如需要将数据分为N类：`lecture 11`

* One-Versus-All (OVA) Decomposition： 
	* 原理：每次拿一个类别和其他所有类别进行比较，需要进行N次比较，获得N个模型。然后依次问N个模型，是否是某这一类。
	* 缺点：有可能多个模型同时回答yes or no。
* One-versus-one (OVO) Decomposition
	* 原理：每次进行两类数据的比较，获得C_N_2个模型，最终结果由这些模型投票而定。
	* 缺点：模型更加零散，而且投票结果可能达成平局。

逻辑回归在多元分类中有较好的应用，因为它给出了置信度，便于按照阈值进行进一步判断。

###非线性分类问题

这里主要说的是用线性分类器解决非线性分类问题`lecture 12`。

主要的用法还是直接使用投影，将数据投影到高维空间，原来非线性可分的问题就有了线性可分的可能性。

##常见误区

###overfitting

对于学习了一些机器学习技法的同学来说来说，overfitting应该是最容易踩的坑。表现为训练结果非常好，但是实际处理问题却非常糟糕（good `E(in)`，bad `E(out)`）。林老师铺垫了多个lecture，在`lecture 13`, 还是将这个事情讲的比较清楚了。

overfitting产生的原因：

* 数据量太少：overfitting的根源，数据越少，越容易取得漂亮的结果，而`E(out)`较大的风险越大（前面多个lecture论述）。
* 模型复杂度过高：使用过于复杂的数据来训练。
* Stochastic Noise：如果数据本身存在一些错误。
* Deterministic Noise：如果数据是一个非常高维的复杂数据，如果数据量不到一定的量级，对于模型来讲，它存在确定性噪音。

如何避免overfitting，林老师给出如下建议：

* start from simple model
* data cleaning/pruning
* data hinting
* regularization
* validation

我个人的看法是，

* 模型复杂度高，表达能力强，并不是overfitting的根源。复杂的模型需要与这个模型相符的数据量才能驾驽。我们需要根据VC bound，计算出需要的数据量。
* 如果存在Stochastic Noise，在噪音不大的情况下，需要的数据量就更加多。如果噪音实在太多，要么想办法去除噪音，要么就只能降低模型复杂度。
* Deterministic Noise个人认为纯粹就是数据量不够导致的，降低模型复杂度是退而求其次的做法。
* 这里一直没有讨论的是复杂模型的计算负担，如果计算能力上不来，在训练不充分的情况下，`E（in）`与`E(out)`效果都不会很好。
* regularization也是在降低模型复杂度，它主要存在的意义还是在于，调整对与模型的改动较少，可以在退而求其次的过程中，较为方便的进行调整。

###关于Regularization 

`lecture 14`介绍了Regularization的内容。Regularization主要还是在做降低模型复杂度操作，相当于将训练向量`w`加上了一个约束。

* L1 Regularizer 相当于限制了`w`的同时，还限制了它的有效分量个数。训练结果往往较多分量的取值为0。
* L2 Regularizer 容易优化，相当于限制了w的向量长度，也就是限制了特别长的分量，分量的值较为均衡。

Regularization因为降低了模型的复杂度，可以显著降低overfitting带来的影响，降低`E(out)`，但是也能导致`E(in)`增加。虽然我们看重的是`E(out)`，但是整体质量下降的情况下，`E(out)`与`E(in)`接近的意义也在下降。因此，个人认为，治本的做法应该还是更合适的模型、更大的训练数据量。

##开展机器学习

开展机器学习，首先需要避开前一节提到的各种坑。

* 评估问题的复杂度（lecture 3 ~ 7）。
* 循序渐进的使用机器学习技法（lecture 8~14）。
* 科学的验证结果，避免overfitting（lecture 15）。


###验证

`lecture 15`给出了科学的结果验证方式，避免偷看试题影响评估，造成overfitting。

* 使用Validation Set `D(val)`：将数据分出一部分，在训练时不要使用，然后用`D(val)`验证训练结果。推荐数据量N/5
* Leave-One-Out Estimate： 将数据每次取出一个当做验证数据，其他的当做训练数据。
* V-fold Cross Validation： 相当于一个组合，将整体数据分成V份，轮流当做验证数据。推荐V=10

###林老师的忠告

* Occam’s Razor： simple is Better
* Avoid Sampling Bias：取样尽量全面
* Visual Data Snooping：不要窥视数据，保证测试数据的纯洁。

##小结

林老师的机器学习基石课程是一门非常基础的课程，由浅入深的介绍了机器学习基本原则。课程内容图文并茂，除了深入浅出的解析理论推导，林老师还特别注重对基本算法的几何意义、物理意义进行类比说明，加深印象和理解。

课程一步一步的引出来VC bound的概念，而不是一上来就进行公式推导。VC bound从理论上给出了`E(in)`和`E(out)`接近的条件。为某一个机器学习，需要多大数据量的数据，以保证良好的效果给出了理论指导。同时，VC bound 也很自然的解释了overfitting出现的原因，反复强调了避免overfitting的重要性。

课程给出的三个机器学习技法也是非常基础和实用的，现阶段逻辑回归还在百度有广泛的应用。难得的是三个技法之间还有反复的类比，增强学生的理解。


##感谢
* 感谢喜欢讲故事、带着台湾腔的`林轩田`老师带来的精彩课程。
* 感谢《机器学习基石》的组织者、推动者、教导员：`金泉`（许校长）、`振广`（朱老师）、`胡光`（胡老师）。
* 感谢我的家人在我在家学习的时候，给予的细心照顾。
* 感谢一起共同学习、互相帮助的同学们。

（待补图）


